{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning the GPT-2 Model \n",
    "\n",
    "This notebook explores the GPT-2 model released by OpenAI on February 14th, 2019. This blog post 'Better Language Models and Their Implications' https://openai.com/blog/better-language-models/, discusses the project.\n",
    "\n",
    "Due to concerns over the effectiveness of their model (this reasoning bears questioning), they have only released a smaller version (117M parameters vs 1.5B parameters) of their larger model. See their reasoning [here](https://openai.com/blog/better-language-models/#releasestrategy). They also did not release any of the training software they used to produce the full GPT-2 model.\n",
    "\n",
    "Data: The GPT-2 model was trained on a corpus of 8 million webpages (40GB of text) scraped from outgoing reddit links with Karma > 3. This if a form of human/collaborative filtering, and supposed indicator of \"quality\" of the outgoing links. See their data curation approach [here](https://openai.com/blog/better-language-models/#fn1).  \n",
    "\n",
    "Paper: [Language Models are Unsupervised Task Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "Code: https://github.com/openai/gpt-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning vs. Retraining\n",
    "\n",
    "Training a 1.5B parameter model requires an enormous corpus of text (8M webpages yielding 40GB of plain text). Even with a reduced 117M parameter GPT-2, training this architecture on a small dataset (where data is smaller than the number of parameters) reduces the networks ability to generalize and likely results in overfitting. Fine-tuning a network means taking that high-parameter, pretrained network, and continuing training (via backpropagation) with our new, smaller dataset. The caveat here is that the new dataset needs to resemble/be similar in kind to the original training data.\n",
    "\n",
    "Even if we wanted to train GPT-2 architecture from scratch, OpenAI has not provided access to the training data or training code.\n",
    "\n",
    "The example below wraps up the gpt-2 fine-tuning process in a very simple interface, installed via pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt-2-simple\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
      "Collecting regex (from gpt-2-simple)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/28/ff0d0936a31f15a0879caf6dac1f1cbaab1fc7b9e8baf8a1d5a70380fb22/regex-2020.11.13-cp37-cp37m-manylinux2010_x86_64.whl (667kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 15.7MB/s eta 0:00:01     |█████████▉                      | 204kB 15.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gpt-2-simple) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/.local/lib/python3.7/site-packages (from gpt-2-simple) (4.32.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from gpt-2-simple) (1.17.1)\n",
      "Collecting toposort (from gpt-2-simple)\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/7d/55784e894ee0cde2474fb977ffd1651e74e840a9f92e1d847f7e3115d5ec/toposort-1.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (2019.6.16)\n",
      "Building wheels for collected packages: gpt-2-simple\n",
      "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp37-none-any.whl size=23579 sha256=a365cdb067fcc649057986eaa255d03370aedf921dd56cfcb66ac45aeb77e444\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
      "Successfully built gpt-2-simple\n",
      "Installing collected packages: regex, toposort, gpt-2-simple\n",
      "\u001b[33m  WARNING: The script gpt_2_simple is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed gpt-2-simple-0.7.1 regex-2020.11.13 toposort-1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt-2-simple --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"124M\"\n",
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              'script.txt',\n",
    "              model_name=model_name,\n",
    "              steps=300)#1000)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities\n",
    "\n",
    "- Explore different finetuning texts (of your choice) and their results.\n",
    "  - Observe how the size of the text (num characters) relate to the number of parameters of the GPT-2 model you are working with.\n",
    "  - Observe the loss \n",
    "  - Look for over-fitting (loss at or near 0)\n",
    "- Try the other sizes of published models, how do they change the complexity/intelligibility of generated text. gpt_2_simple wraps the \"small\" 124M and \"medium\" 355M models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Example is from [https://github.com/minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
